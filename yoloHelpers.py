#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jun 23 11:46:19 2022

@author: hnagaty
"""

import torch
import numpy as np
from itertools import repeat
import torch.nn as nn
import matplotlib.pyplot as plt
import torchvision.transforms.functional as F

S = 7 # An image is divided into SxS grid cells
B = 2 # number of boxes per grid cell
C = 20 # number of classes

voc_classes = ["background", "aeroplane", "bicycle", "bird", "boat",
               "bottle", "bus", "car", "cat", "chair", "cow", "diningtable",
               "dog", "horse", "motorbike", "person", "pottedplant", "sheep",
               "sofa", "train", "tvmonitor"]

def IOU2(box_1, box_2, device=torch.device('cpu'), use_float64=False):
    """
    NOT USED
    Got this from
    https://github.com/zzzheng/pytorch-yolo-v1/blob/master/utils.py
    
    Tensor version of calc_IOU()
    compute IOU between two bounding boxes
    :param box_1: Detection x, y, w, h image coordinates in [0, 1]
    :param box_2: GroundTruth x, y, w, h image coordinates in [0, 1]
    return:
    """

    x_min_1 = torch.clamp((abs(box_1[0]) - abs(box_1[2]) / 2), 0, 1).to(device)
    x_max_1 = torch.clamp((abs(box_1[0]) + abs(box_1[2]) / 2), 0, 1).to(device)
    y_min_1 = torch.clamp((abs(box_1[1]) - abs(box_1[3]) / 2), 0, 1).to(device)
    y_max_1 = torch.clamp((abs(box_1[1]) + abs(box_1[3]) / 2), 0, 1).to(device)

    x_min_2 = torch.clamp((box_2[0] - box_2[2] / 2), 0, 1).to(device)
    x_max_2 = torch.clamp((box_2[0] + box_2[2] / 2), 0, 1).to(device)
    y_min_2 = torch.clamp((box_2[1] - box_2[3] / 2), 0, 1).to(device)
    y_max_2 = torch.clamp((box_2[1] + box_2[3] / 2), 0, 1).to(device)


    # z = torch.tensor(0, dtype=torch.float).to(device)
    z = torch.tensor(0.).to(device)
    if use_float64:
        z = z.double()

    a = torch.min(x_max_1, x_max_2)
    b = torch.max(x_min_1, x_min_2)
    c = torch.min(y_max_1, y_max_2)
    d = torch.max(y_min_1, y_min_2)

    overlap_width = torch.max(a-b, z)
    overlap_height = torch.max(c-d, z)
    overlap_area = overlap_width * overlap_height

    union_area = (x_max_1 - x_min_1) * (y_max_1 - y_min_1) \
                 + (x_max_2 - x_min_2) * (y_max_2 - y_min_2) \
                 - overlap_area
    intersection_over_union = overlap_area / union_area
    return intersection_over_union


def IOU(boxes_preds, boxes_labels, box_format='midpoint'):
    """
    Got this from https://www.kaggle.com/code/vexxingbanana/yolov1-from-scratch-pytorch/notebook
    Calculates intersection over union
    
    Parameters:
        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)
        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)
        box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively.
    
    Returns:
        tensor: Intersection over union for all examples
    """
    # boxes_preds shape is (N, 4) where N is the number of bboxes
    #boxes_labels shape is (n, 4)
    
    if box_format == 'midpoint':
        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2
        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2
        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2
        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2
        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2
        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2
        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2
        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2
        
    if box_format == 'corners':
        box1_x1 = boxes_preds[..., 0:1]
        box1_y1 = boxes_preds[..., 1:2]
        box1_x2 = boxes_preds[..., 2:3]
        box1_y2 = boxes_preds[..., 3:4] # Output tensor should be (N, 1). If we only use 3, we go to (N)
        box2_x1 = boxes_labels[..., 0:1]
        box2_y1 = boxes_labels[..., 1:2]
        box2_x2 = boxes_labels[..., 2:3]
        box2_y2 = boxes_labels[..., 3:4]
    
    x1 = torch.max(box1_x1, box2_x1)
    y1 = torch.max(box1_y1, box2_y1)
    x2 = torch.min(box1_x2, box2_x2)
    y2 = torch.min(box1_y2, box2_y2)
    
    #.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0
    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)
    
    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))
    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))
    
    return intersection / (box1_area + box2_area - intersection + 1e-6)


def read_labels(label_file):
    """
    Will not use this
    Adapted from https://github.com/zzzheng/pytorch-yolo-v1/blob/master/utils.py
    Given a label file in text format, read labels from the file
    It uses labels generated by voc_label.py script
    Format is <class> <x> <y> <w> <h>
    x & y are between 0 and 1
    
    @param label_file: a .txt file
    @return: labels: [list]
    """
    with open(label_file, 'r') as f:
        lines = f.readlines()
        labels = []
        for l in lines:
            l = l.split()
            l = [float(elem) for elem in l]
            l[0] = int(l[0]) # the class is integer
            labels.append(l)
    return labels



def format_label(annotation):
    """
    Formats the labels of VOD detection dataset to a numpy array

    Parameters
    ----------
    annotation : dict
        A dictionary of the annotations (of a single image) as 
        obtained from the xml annotation file.
        Note that this is a single image, and it could have 
        multiple detected objects

    Returns
    -------
    np.array
        The annotation as numpy array
        Its size is n x 5, where n is the number of objects in 
        the image

    """
    
    # I define those functions here since I use them only
    # from within this function
    
    def format_detected_object(obj, img_x, img_y):
        """
        Formats the label of a single detected object
        It converts the class label to a number
        It converts xmin, xmax, ymin & ymax to x, y, w & h
        
        Parameters
        ----------
        obj : dict
            A single detected object. This is a dictionary as returned 
            from the xml annotation file
        img_x & img_y: int
            The image size in pixels.
            This is also normally obtained from the xml annotation

        Returns
        -------
        np array of size 5 (class, x, y, w, h)

        """
        obj_class = voc_classes.index(obj['name'])
        
        xmin = int(obj['bndbox']['xmin'])
        ymin = int(obj['bndbox']['ymin'])
        xmax = int(obj['bndbox']['xmax'])
        ymax = int(obj['bndbox']['ymax'])
        
        x = (xmin + xmax) / 2 / img_x
        y = (ymin + ymax) / 2 / img_y
        w = (xmax - xmin) / img_x
        h = (ymax - ymin) / img_y
        lst = [obj_class, x, y, w, h]
        return lst
        #return np.array(lst)
    
    def make_label_array(boxes):
        """
        Creates a numpy array for the labels
        This is for a single image
        It is a 3D array whose dimensions is S, S, C+5xB
        
        Adapted from code at
        https://sachinsachan.medium.com/yolo-v1-implementation-7c88a33795f4
    
        Parameters
        ----------
        labels : list
            list of labels
            Each element is a list [class, x, y, w, h]
    
        Returns
        -------
        label_array : numpy array
            An array of the labels in the format that is needed by YOLO
        """
        label_matrix = np.zeros([S, S, 5 * B + C])
        for box in boxes:
            cls, x1, y1, w , h,  = box
            loc = [S * x1, S * y1]
            loc_i = int(loc[1])
            loc_j = int(loc[0])
            y1 = loc[1] - loc_i
            x1 = loc[0] - loc_j
            
            if label_matrix[loc_i, loc_j, C] == 0: # if the grid cell doesn't have a bbox
                label_matrix[loc_i, loc_j, C] = 1 # confidence
                label_matrix[loc_i, loc_j, cls] = 1 # class one hot encode
                label_matrix[loc_i, loc_j, C+1:C+5] = x1, y1, h , w # x,y relative to grid cell. w & h relative to picture size
        return label_matrix

    img_x = int(annotation['annotation']['size']['width'])
    img_y = int(annotation['annotation']['size']['height'])
    
    detected_objects = annotation['annotation']['object'] # a list of detected objects
    
    detected_lst = list(map(format_detected_object, detected_objects,
                    repeat(img_x), repeat(img_y)))
    
    # return detected_lst
    # return np.array(detected_lst)
    return make_label_array(detected_lst)


class Yolo(nn.Module):
    '''
    Neural Network Definition
    I followed the authors configuration file in darknet/cfg/yolov1.cfg and
    the diagram in the paper.
    The CNN block is composed of several conv & maxpool layers
    Each conv layer is followed by batch norm, and uses leaky relu activation
    All conv layers has stride=1 (except firsl conv layer), and padding=0
    Note
    The original paper specefies same padding for all layer. However, PyTorch
    doesn't support same padding when stide != 1. So, I used padding=0 in some
    layers and padding=1 in some. I reached this workaround with help from
    https://www.kaggle.com/code/vexxingbanana/yolov1-from-scratch-pytorch/notebook
    '''
    def __init__(self):
        super(Yolo, self).__init__()

        # Feature Extraction Module
        self.CNN = nn.Sequential(
            
            # -- block1 (input layer)
            nn.Conv2d(in_channels=3, out_channels=64, 
                      kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.1),
            # -- Maxpool
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # -- block2
            nn.Conv2d(in_channels=64, out_channels=192, 
                      kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(192),
            nn.LeakyReLU(0.1),
            # -- Maxpool
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # -- block3_a
            nn.Conv2d(in_channels=192, out_channels=128, 
                      kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.1),
            # -- block3_b
            nn.Conv2d(in_channels=128, out_channels=256, 
                      kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.1),  
            # -- block3_c
            nn.Conv2d(in_channels=256, out_channels=256, 
                      kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.1),  
            # -- block3_d
            nn.Conv2d(in_channels=256, out_channels=512, 
                      kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.1),
            # -- MaxPool
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # -- block4_a:1
            nn.Conv2d(in_channels=512, out_channels=256, 
                      kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.1),
            # -- block4_b:1
            nn.Conv2d(in_channels=256, out_channels=512, 
                      kernel_size=1, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.1),
            # -- block4_a:2
            nn.Conv2d(in_channels=512, out_channels=256, 
                      kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.1),
            # -- block4_b:2
            nn.Conv2d(in_channels=256, out_channels=512, 
                      kernel_size=1, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.1),
            # -- block4_a:3
            nn.Conv2d(in_channels=512, out_channels=256, 
                      kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.1),
            # -- block4_b:3
            nn.Conv2d(in_channels=256, out_channels=512, 
                      kernel_size=1, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.1),
            # -- block4_a:4
            nn.Conv2d(in_channels=512, out_channels=256, 
                      kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.1),
            # -- block4_b:4
            nn.Conv2d(in_channels=256, out_channels=512, 
                      kernel_size=1, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.1),
            # -- block4_c
            nn.Conv2d(in_channels=512, out_channels=512, 
                      kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.1),
            # -- block4_d
            nn.Conv2d(in_channels=512, out_channels=1024, 
                      kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1),
            # -- MaxPool
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # -- block5_a:1
            nn.Conv2d(in_channels=1024, out_channels=512, 
                      kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.1),
            # -- block5_b:1
            nn.Conv2d(in_channels=512, out_channels=1024, 
                      kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1),
            # -- block5_a:2
            nn.Conv2d(in_channels=1024, out_channels=512, 
                      kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.1),
            # -- block5_b:2
            nn.Conv2d(in_channels=512, out_channels=1024, 
                      kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1),
            # -- block5_c
            nn.Conv2d(in_channels=1024, out_channels=1024, 
                      kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1),  
            # -- block5_d
            nn.Conv2d(in_channels=1024, out_channels=1024, 
                      kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1),

            # -- block6_a
            nn.Conv2d(in_channels=1024, out_channels=1024, 
                      kernel_size=3, stride=1, padding=0),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1),
            # -- block6_b -- same as 6a
            nn.Conv2d(in_channels=1024, out_channels=1024, 
                      kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1))
        
        self.FC = nn.Sequential(
           nn.Flatten(),
           nn.Linear(1024 * S * S, 4096),
           nn.Dropout(0.5),
           nn.LeakyReLU(0.1),
           nn.Linear(4096, S * S * (B * 5 + C)))
        
    def forward(self, x):
        x = self.CNN(x)
        x = self.FC(x)
        x  = x.view(x.size(0), S, S,  B * 5 + C)
        return x


class YoloLoss(nn.Module):
    """
    Got this from https://www.kaggle.com/code/vexxingbanana/yolov1-from-scratch-pytorch/notebook
    Calculate the loss for yolo (v1) model
    """

    def __init__(self): #  , S=7, B=2, C=3): Here I use S, B & C as global variables
        super(YoloLoss, self).__init__()
        self.mse = nn.MSELoss(reduction="sum")

        """
        S is split size of image (in paper 7),
        B is number of boxes (in paper 2),
        C is number of classes (in paper 20, in dataset 3),
        """
        self.S = S
        self.B = B
        self.C = C

        # These are from Yolo paper, signifying how much we should
        # pay loss for no object (noobj) and the box coordinates (coord)
        self.lambda_noobj = 0.5
        self.lambda_coord = 5

    def forward(self, predictions, target):
        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted
        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)

        # Calculate IoU for the two predicted bounding boxes with target bbox
        iou_b1 = IOU(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])
        iou_b2 = IOU(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])
        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)

        # Take the box with highest IoU out of the two prediction
        # Note that bestbox will be indices of 0, 1 for which bbox was best
        iou_maxes, bestbox = torch.max(ious, dim=0)
        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i

        # ======================== #
        #   FOR BOX COORDINATES    #
        # ======================== #

        # Set boxes with no object in them to 0. We only take out one of the two 
        # predictions, which is the one with highest Iou calculated previously.
        box_predictions = exists_box * (
            (
                bestbox * predictions[..., self.C + 6:self.C + 10]
                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]
            )
        )

        box_targets = exists_box * target[..., self.C + 1:self.C + 5]

        # Take sqrt of width, height of boxes to ensure that
        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(
            torch.abs(box_predictions[..., 2:4] + 1e-6)
        )
        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])

        box_loss = self.mse(
            torch.flatten(box_predictions, end_dim=-2),
            torch.flatten(box_targets, end_dim=-2),
        )

        # ==================== #
        #   FOR OBJECT LOSS    #
        # ==================== #

        # pred_box is the confidence score for the bbox with highest IoU
        pred_box = (
            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]
        )

        object_loss = self.mse(
            torch.flatten(exists_box * pred_box),
            torch.flatten(exists_box * target[..., self.C:self.C + 1]),
        )

        # ======================= #
        #   FOR NO OBJECT LOSS    #
        # ======================= #

        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])
        #no_object_loss = self.mse(
        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),
        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),
        #)

        no_object_loss = self.mse(
            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),
            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),
        )

        no_object_loss += self.mse(
            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),
            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)
        )

        # ================== #
        #   FOR CLASS LOSS   #
        # ================== #

        class_loss = self.mse(
            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),
            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),
        )

        loss = (
            self.lambda_coord * box_loss  # first two rows in paper
            + object_loss  # third row in paper
            + self.lambda_noobj * no_object_loss  # forth row
            + class_loss  # fifth row
        )

        return loss

def pred_to_box(pred, conf_threshold = 0.5, 
                img_x = 1, img_y = 1,
                label_decode=False):
    """
    Converts YOLO tensor to bounding box (PASCAL VOC) format.
    The result can be used to draw bounding boxes or to calculate
    the mAP.
    This is a slow, non-vectorised function. 
    It can work okay for a single or few infered images, but not
    efficient for checking mAP during training.
    
    Parameters
    ----------
    pred : numpy array of dimension (S x S x C + B * 5)
           This repesents the prediction for a single image
    conf_threshold : float, optional
        DESCRIPTION. The confidence threshold below which I ignore the bounding box.
        The default is 0.5.
    img_x, img_y : int, optional
        Image width & height.
        This is needed to convert relative co-ordinates to image pixel cordinates 
        The default is 1. That means no conversion is done
    label_decode: Boolean
        If True, then convert label class number to label.
        It uses the voc_classes global variable

    Returns
    -------
    dict
        DESCRIPTION.

    """
    
    bboxes = [] # bounding boxes
    scores = [] # confidence score for each box
    labels = [] # class number for each box

    for loc_i in range(pred.shape[-2]):
        for loc_j in range(pred.shape[-3]):
            predX = pred[loc_i, loc_j] # a single grid cell prediction
            # for a single box, 
            # apply this to all boxes
            boxesB = predX[C:].reshape([B, 5]) # reshape bounding boxes to B x 5
            # set confidence to zero if any x, y, w or h is < 0
            boxesB[boxesB.min(axis = -1) < 0] = 0
            boxC = boxesB[:,0] # the confidence for each box
            # Only consider a box if confidence is greater than threshold
            # and if all box predictors are positive
            if boxC.max() > conf_threshold:
                boxI = boxC.argmax() # the box of interest; the one with the higher confidence
                boxDims = boxesB[boxI, 1:] # dimensions of the box, x, y, w, h
                x, y, h, w = boxDims # WHY WHY WHY
                # convert x & y to picture relative dims
                xB, yB = (loc_j + x) / S, (loc_i + y) / S # x,y relative to the image, rather than the grid cell
                xC, yC = int(xB * img_x), int(yB * img_y) # center x & y in pixels
                x_min, x_max = xC - int(w/2 * img_x), xC + int(w/2 * img_x)
                y_min, y_max = yC - int(h/2 * img_y), yC + int(h/2 * img_y)
                bboxes.append([x_min, y_min, x_max, y_max])
                # bboxes.append([xB, yB, w, h]) # for debugging
                scores.append(boxC.max())
                label = predX[0:C].argmax()
                if label_decode:
                    label = voc_classes[label]
                labels.append(label)
                    
    return {"boxes": bboxes,
            "labels": labels,
            "scores": scores}

def show(imgs):
    if not isinstance(imgs, list):
        imgs = [imgs]
    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)
    for i, img in enumerate(imgs):
        img = img.detach()
        img = F.to_pil_image(img)
        axs[0, i].imshow(np.asarray(img))
        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])